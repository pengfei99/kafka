# 2 Installing and configuring kafka by using zookeeper

Before kafka 2.8.0, zookeeper is the mandatory dependency for ensuring the consensus between different nodes.

**Apache Zookeeper** is a distributed configuration and synchronization service. Kafka uses ZooKeeper for:
- leadership election of Kafka Broker to elect **Controller**: The first arrived broker creates /kafka/controller znode in zk.
   The other broker will fail because it already exists.
- partition replication leader/follower election
- discover new coming brokers
- stores basic metadata in Zookeeper such as information about topics(topic-partition pairs), brokers, consumer offsets (queue readers) and so on.
- ETc.

Since all the critical information is stored in the Zookeeper and it normally replicates this data across its ensemble,
failure of Kafka broker / Zookeeper does not affect the state of the Kafka cluster. Kafka will restore the state,
once the Zookeeper restarts. This gives zero downtime for Kafka. The leader election between the Kafka broker is also
done by using Zookeeper in the event of leader failure.

To learn more about ZooKeeper: [[employes:pengfei.liu:data_science:zookeeper:start|ZooKeeper docs]]

## 2.1 Prerequisite

- Install JDK : Kafka needs JDK or JRE to run. So the first step is to install jdk.
- Install zookeeper



## 2.2 Hardware requirement

At least 4GB of RAM on the server. Installations without this amount of RAM may cause the Kafka service to fail,
with the Java virtual machine (JVM) throwing an “Out Of Memory” exception during startup.


## 2.3 Install kafka

### 2.3.1 Creating a User for Kafka
```shell
# create a system user Kafka with a home dir
useradd -r kafka -m

# add Kafka to sudoer group
usermod -aG wheel kafka

# login as kafka
su -l kafka

```

### 2.3.2 Downloading and Extracting the Kafka Binaries
Download the kafka binary from http://kafka.apache.org/downloads.html

Note that Kafka uses Scala, and it's binary is compiled based on certain version of Scala. For example, kafka_2.12-3.1.0
means this Kafka version 3.1.0 is compiled based on scala 2.12; kafka_2.11-2.5.0 means this kafka of version 2.5.0 is
compiled based on scala 2.11. You don't need to install scala on your server, because the binary is already compiled
in .class. You only need right version of scala to run it.

Then place the binary in your server. I put it under "/opt/kafka" 

```shell
mkdir /opt/kafka

cd /opt/kafka

# extract the binary
tar -xzvf kafka_2.12-2.5.0.tgz

# change the name
mv kafka_2.12-2.5.0 kafka-2.5.0

# check the result
[root@localhost kafka-2.5.0]# pwd
/opt/kafka/kafka-2.5.0

```

## 2.4 Configuring the Kafka Server

The main config file of a Kafka broker is on the **server.properties** file. The default config is enough to run a
standalone Kafka broker as a test server. But I still want to highlight some key configuration attributes.

```text
【broker.id】
每个broker都必须自己设置的一个唯一id，可以在0~255之间. A good guideline is to set this value to something intrinsic to the host.
For example, if your hostnames contain a unique number (such as host1.example.com , host2.example.com , etc.),
that is a good choice for the broker.id value.

【log.dirs】
这个极为重要，kafka的所有数据就是写入这个目录下的磁盘文件中的，如果说机器上有多块物理硬盘，那么可以把多个目录挂载到不同的物理硬盘上，
然后这里可以设置多个目录，这样kafka可以数据分散到多块物理硬盘，多个硬盘的磁头可以并行写，这样可以提升吞吐量。ps：多个目录用英文逗号分隔.
If more than one path is specified(e.g. log.dirspath1,path2), the broker will store partitions on them in a
“least-used” fashion with one partition’s log segments stored within the same path. Note that the broker will
place a new partition in the path that has the least number of partitions currently stored in it, not the least
amount of disk space used in the following situations

【zookeeper.connect】
连接kafka底层的zookeeper集群的. The default configuration is localhost:2181.

【Listeners】
broker监听客户端发起请求的ip,端口号，默认是9092. It will get the value returned from **java.net.InetAddress.getCanonicalHostName()
if not configured**. The port number can be set to any available port by changing the port configuration parameter.
Keep in mind that if a port lower than 1024 is chosen, Kafka must be started as root. Running Kafka as root is not
a recommended configuration.

```


```shell
# open the config file
cd /opt/kafka/kafka_2.11-1.0.0/config
vim server.properties

```

The following is an example of minimum configuration

```text
#in a single server with default zookeeper
broker.id=0
log.dirs=/tmp/kafka-logs
zookeeper.connect=localhost:2181

#in a cluster environment with multiple zookeeper instances
zookeeper.connect=hadoop-nn.pengfei.org:2181,hadoop-dn1.pengfei.org:2181,hadoop-dn2.pengfei.org:2181
broker.id=1 (2,3 in other two machines)
log.dirs=fs_on_disk1,fs_on_disk2

```

The above config is the minimum config to run kafka, you may need to dig more.



## 2.5 Creating Systemd Unit Files and Starting the Kafka Server

We need to create the systemd service file for Kafka. As Kafka requires ZooKeeper to manage its cluster state and
configuration. We need to make sure we have a systemd service file for Zookeeper first. Suppose we have it
in `/etc/systemd/system/zk.service`

Our Kafka systemd file looks like this:
```shell
[Unit]
Requireszk.service
Afterzk.service

[Service]
Typeforking
WorkingDirectory/opt/kafka/kafka-2.5.0
Userkafka
Groupkafka
ExecStart/opt/kafka/kafka-2.5.0/bin/kafka-server-start.sh /opt/kafka/kafka-2.5.0/config/server.properties > /var/log/kafka/kafka.log 2>&1
ExecStop/opt/kafka/kafka-2.5.0/bin/kafka-server-stop.sh
Restarton-abnormal

[Install]
WantedBymulti-user.target

```

The Unit part will ensure that zookeeper gets started automatically when the kafka service starts.

Run the daemon
```shell
sudo systemctl start/stop/status kafka

# run daemon at system startup
sudo systemctl enable kafka

```


## 2.6 Testing the Installation 

To test the kafka server (with one broker). Make sure the zookeeper is running before you run this command.

```shell
sh bin/kafka-server-start.sh config/server.properties

```

### 2.6.1 Create a topic 

Create a topic with one partition and three replication

```shell
#create a topic
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --create --zookeeper hadoop-nn.bioaster.org --replication-factor 3 --partitions 1 --topic test-topic
Created topic "test-topic".

#check status of a topic
$ sh kafka-topics.sh --describe --zookeeper hadoop-nn.bioaster.org --topic test-topic
Topic:test-topic	PartitionCount:1	ReplicationFactor:3	Configs:
	Topic: test-topic	Partition: 0	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1
```


Create a topic with three partitions and three replication

```shell
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 3 --topic Hello-Kafka

Created topic "Hello-Kafka".

#get status of the topic
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --describe --zookeeper hadoop-nn.bioaster.org:2181 --topic Hello-Kafka
Topic:Hello-Kafka	PartitionCount:3	ReplicationFactor:3	Configs:
	Topic: Hello-Kafka	Partition: 0	Leader: 1	Replicas: 1,2,3	Isr: 1
	Topic: Hello-Kafka	Partition: 1	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1
	Topic: Hello-Kafka	Partition: 2	Leader: 3	Replicas: 3,1,2	Isr: 3,1,2

```

List all topic in one broker

```shell
[hadoop@CCLinDataWHD01 bin]$ sh kafka-topics.sh --list --zookeeper hadoop-nn.bioaster.org:2181
Hello-Kafka
test-topic

```

### 2.6.2 Start producer to send messages

```shell
# In the broker-list, you could put one or more kafka brokers
# In this example, we use only one, the name of the broker is defined in server.properties  
[hadoop@CCLinDataWHD01 bin]$ sh kafka-console-producer.sh --broker-list hadoop-nn.bioaster.org:9092 --topic Hello-Kafka
#now you are in producer consol, all the text you enter below will be published in topic Hello-Kafka
>hello
>my first message
>my second message
>my third message

```

### 2.6.3 Start two consumers to receive messages

```shell
###in server hadoop-dn1.bioaster.org
hadoop@CCLinDataWHD02 bin]$ sh kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic Hello-Kafka --from-beginning
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
hello
my first message
my second message
my third message
my fouth message
my fifth message


###in server hadoop-dn2.bioaster.org
[hadoop@CCLinDataWHD03-0 bin]$ sh kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic Hello-Kafka --from-beginning
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
hello
my third message
my first message
my second message
my fouth message
my fifth message


##You could notice, if your consumer connecter after many message has been published
##the order may not be correct. But the latest message will be correct

```

### 2.6.4 Close the test

When you are done testing, press CTRL+C to stop the producer and consumer scripts.

To delete the topic use the following command

```shell
bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic pengfeiTest
```

### 2.7 Setting Up a Multi-Node Cluster

There are three possible scenarios:
- Multi brokers on single servers
- Single brokers on multi-servers
- Multi brokers on multi-servers

### 2.7.1 Multi brokers on single servers

For setting up multiple brokers on a single node, different server property files are required for each broker.
Each property file will define unique, different values for the following properties:
- broker.id
- port
- log.dir

For example, if we want to run three brokers on a single server, we need to create three config files
- server1.properties,
- server2.properties,
- server3.properties.

The content of server1.properties:
```text
broker.id1
listeners  PLAINTEXT://your.host.name:9093
log.dir/tmp/kafka-logs-1
```

The content of server2.properties:
```text
broker.id2
listeners  PLAINTEXT://your.host.name:9094
log.dir/tmp/kafka-logs-2

```

The content of server3.properties:
```text
broker.id3
listeners  PLAINTEXT://your.host.name:9095
log.dir/tmp/kafka-logs-3
```

We can run the three instances of the broker with the following command

```shell
#run broker1
bin/kafka-server-start.sh config/server1.properties

#run broker2
bin/kafka-server-start.sh config/server1.properties

#run broker3
bin/kafka-server-start.sh config/server1.properties
```

### 2.7.2 Single broker on multi nodes

This is very simple, all these brokers can have the exact same configuration except the **broker.id**.

Zookeeper configuration

If you want your Kafka broker to connect to multiple zookeeper servers. In zookeeper.connect, you can use
comma-separated string listing the IP addresses and port numbers of all the ZooKeeper instances.

For example

```shell
//Kafka uses three zk instances
zookeeper.connect  zk1.pengfei.org:2181,zk2.pengfei.org:2181,zk3.pengfei.org:2181
```

## 2.8 Restricting the Kafka User

```shell
# Remove the kafka user from the sudo group:
sudo gpasswd -d kafka wheel

# To further improve your Kafka server’s security, lock the kafka user’s password using the passwd command. This makes sure that nobody can directly log into the server using this account:
sudo passwd kafka -l

# At this point, only root or a sudo user can log in as kafka by typing in the following command:
sudo su - Kafka

# In the future, if you want to unlock it, use passwd with the -u option:
sudo passwd kafka -u


```

## 2.9 Basic topic operations (Optional)

### 2.9.1 List all topics and show details of a topic


```shell

$ sh kafka-topics.sh --list --zookeeper hadoop-nn.bioaster.org:2181
Hello-Kafka
test-topic

#show detail of topic test-topic
$ sh kafka-topics.sh --describe --zookeeper hadoop-nn.bioaster.org:2181 --topic test-topic
Topic:test-topic	PartitionCount:1	ReplicationFactor:3	Configs:
	Topic: test-topic	Partition: 0	Leader: 2	Replicas: 2,3,1	Isr: 2,1,3

#we know that it has one partition and 3 replicas
```

### 2.9.2 Change the partition number of a topic

```shell
#Now we want to change the partition from one to two
$ sh kafka-topics.sh --zookeeper hadoop-nn.bioaster.org:2181 --alter --topic test-topic --partitions 2


$ sh kafka-topics.sh --describe --zookeeper hadoop-nn.bioaster.org:2181 --topic test-topic
Topic:test-topic	PartitionCount:2	ReplicationFactor:3	Configs:
	Topic: test-topic	Partition: 0	Leader: 2	Replicas: 2,3,1	Isr: 2,1,3
	Topic: test-topic	Partition: 1	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1
```

### 2.9.3 Delet a topic


```shell
$ sh kafka-topics.sh --zookeeper hadoop-nn.bioaster.org:2181 --delete --topic test-topic


$ sh kafka-topics.sh --list --zookeeper hadoop-nn.bioaster.org:2181
Hello-Kafka
```
## 2.10 Kafka runs starts and runs the zookeeper (Optional)

if your zookeeper cluster already running or controlled by zookeeper, you don't need to change the config
of /opt/kafka/kafka_2.11-1.0.0/config/zookeeper.properties

if you want to use kafka to control the zookeeper daemon, you need to edit the zookeeper.properties

Here is an example

```shell
# the directory where the snapshot is stored.
dataDir /opt/zookeeper/zookeeper-3.4.10/data
# the port at which the clients will connect
clientPort 2181
# disable the per-ip limit on the number of connections since this is a non-production config
maxClientCnxns 0

# The number of milliseconds of each tick
tickTime 2000

# The number of ticks that the initial synchronization phase can take
initLimit 10

# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit 5

#zoo servers
server.1 hadoop-nn.pengfei.org:2888:3888
server.2 hadoop-dn1.pengfei.org:2888:3888
server.3 hadoop-dn2.pengfei.org:2888:3888

```


It's the same as the zoo.cfg in zookeeper

```shell
# The number of milliseconds of each tick
tickTime 2000
# The number of ticks that the initial 
# synchronization phase can take
#initLimit10
initLimit 5
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
#syncLimit5
syncLimit 5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just 
# example sakes.
dataDir /opt/zookeeper/zookeeper-3.4.10/data
# the port at which the clients will connect
clientPort 2181
# the maximum number of client connections.
# increase this if you need to handle more clients
#maxClientCnxns60
#
# Be sure to read the maintenance section of the 
# administrator guide before turning on autopurge.
#
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount3
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval1

server.1 hadoop-nn.pengfei.org:2888:3888
server.2 hadoop-dn1.pengfei.org:2888:3888
server.3 hadoop-dn2.pengfei.org:2888:3888

```

To run the zookeeper daemon in kafka

```shell
cd /opt/kafka/kafka_2.11-1.0.0
sh bin/zookeeper-server-start.sh config/zookeeper.properties
```







