# 3. Kafka producer

## 3.1 Architecture

Kafka producer and the various components is responsible of sending/delivering the messages to the Kafka broker
(topic -partition).

Below figure shows the internal architecture of Kafka producer
![kafka_producer_arch](../images/kafka_producer_architecture.png)

### 3.1.1 Important components of the Kafka producer

- **Send()**: It adds the record to a buffer of pending records to be sent and returns immediately. This buffer of
              pending records helps to batch together individual records for efficiency.
- **Interceptor(optional)**:  Kafka Clients support pluggable interceptors to examine (and potentially modify) messages.
              For example, Confluent Control Center uses an interceptor that collects statistics on incoming and
              outgoing messages to provide Stream Monitoring.
- **Serializer**: It helps serialize the key and the value objects that the user provides with their ProducerRecord
              into bytes.
- **Partitioner**: Partition assignment to every ProducerRecord is done by the Partitioner. There are two types of
              partitioning strategies:
   - **Round Robin Partitioning**: This is used when the key is Null in ProducerRecord. Partitions are assigned in Round
               Robin fashion.
   - **Hash Key-based Partitioning**: This is used when ProducerRecord has a key. Partitions are determined based on
               the hash of the key for a particular message/value.
   - **Topic Partition** can also be mentioned directly in the ProducerRecord. In that case, Round Robin and Hash Key
        Partitioning will not make any effect.
- **Buffer**: A memory buffer of the unsent records is maintained by the producer for each partition. The size of
           the buffer can be set by the batch.size config.
- **I/O Threads**: The background I/O thread is responsible for turning the records in the buffer into requests and
            transmitting them to the Kafka broker.

## 3.2 Workflow of producer for sending a message

### Step1:
Creating a **ProducerRecord** which has
- the topic-name
- partition
- Timestamp
- the message-value
- the message-key

Then call method **send()** to send the ProducerRecord


### Step2: Interceptor (Optional)

If you want to monitor or modify the **ProducerRecord**, you can add a Interceptor before serializer.ProducerRecord

### Step3: Serializer

Now, the ProducerRecord arrives at the serializer where the key and value are serialized into ByteArrays. The serialization
facilitate the data exchange over the network.

### Step4: Partitioner
Now, the ProducerRecord arrives at the partitioner. If the partition is specified in the ProducerRecord, then the
partitioner will return the same, otherwise, it will choose a partition for the message-value based on the
partitioning strategy (Round Robin, Hash Key, or Custom Partitioning).

### Step5: Batch Buffer memory

For each partition of each topic, there is a corresponding batch buffer memory. Once the partition of ProducerRecord
is determined, the producer adds the **message-value (without other fields)** to the corresponding batch buffer
(the same topic and partition).

Once the message arrives at the buffer memory, the send() returns and starts processing the next ProducerRecord.

### Step6: I/O thread

A separate **I/O thread** is responsible for sending those batches of records as a request to the Kafka broker. When the
broker successfully receives the messages, I/O thread will return a RecordMetadata object as response and it has the
topic, partition, and offset of the record within the partition.

If the message fails to be received by the broker, I/O thread will return an error as a response, and the producer may
retry sending the message a few more times (No. of retries) before giving up and returning the error.

## 3.3 Important producer configurations

### 3.3.1 Acknowledgements of message

**Acks** configures the number of acknowledgments the producer requires the leader to have received before considering
a request complete. The following settings are allowed:
1. **acks = 0**: No acknowledgment. Messages will be added to the buffer and considered sent. No assurance can be
                 given that the message was received by the broker.
2. **acks = 1**: The producer will wait for the acknowledgment only from the leader broker. No guarantee can be made
                 that the message was received by the follower broker or that the message was replicated properly.
3. **acks = -1 (all)**: The producer will wait for the acknowledgment from the full set in-sync replicas
                 (leaders + followers ). This guarantees that the record was received by all the in-sync brokers
                 and as long as at least one in-sync replica broker remains alive, the message will not be lost.


#### Example:
Suppose we have the following kafka broker setup, and we set **acks = all** in the producer
```text
No. of brokers = 3
Replication Factor = 3
min.insync.replicas = 2 (including leader),
```
As the min.insync.replica is two , we can only tolerate one broker going down. If more than one broker goes down,
the message may get lost and won't get served to the consumer.

#### Trade-Off
Also, setting acks to "1" or "all" may increase the latency since the producer needs to wait for the ack. So we are
facing a trade-off between performance and quality of message delivering. Based on your requirements, you need to choose
one over the other, you can't have them both.


### 3.3.2 Retries and Timeouts -

- retries =(some integer value): It represents the number of times the producer retries sending the message whose
           send() got failed. However, an issue while retrying is that the order of messages/requests may change.
- max.in.flight.requests.per.connection: It defines the max number of requests that can be sent on a connection
            before blocking. Setting this value to 1 means that only one request will be sent at a time thus
            preserving the message order and hence the ordering issue caused by the retry is solved.
            Based on Kafka versions, the recommended value change:
            - max.in.flight.requests.per.connection = 1 (0.11 >= Kafka < 1.1)
            - 5 (Kafka >= 1.1)
